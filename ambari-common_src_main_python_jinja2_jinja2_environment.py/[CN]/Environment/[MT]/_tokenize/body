def _tokenize(self, source, name, filename=None, state=None):
    'Called by the parser to do the preprocessing and filtering\n        for all the extensions.  Returns a :class:`~jinja2.lexer.TokenStream`.\n        '
    source = self.preprocess(source, name, filename)
    stream = self.lexer.tokenize(source, name, filename, state)
    for ext in self.iter_extensions():
        stream = ext.filter_stream(stream)
        if (not isinstance(stream, TokenStream)):
            stream = TokenStream(stream, name, filename)
    return stream
