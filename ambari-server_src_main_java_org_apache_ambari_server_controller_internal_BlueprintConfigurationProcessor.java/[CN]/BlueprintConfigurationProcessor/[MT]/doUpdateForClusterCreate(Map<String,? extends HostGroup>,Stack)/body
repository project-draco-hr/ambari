{
  for (  Map<String,Map<String,PropertyUpdater>> updaterMap : createCollectionOfUpdaters()) {
    for (    Map.Entry<String,Map<String,PropertyUpdater>> entry : updaterMap.entrySet()) {
      String type=entry.getKey();
      for (      Map.Entry<String,PropertyUpdater> updaterEntry : entry.getValue().entrySet()) {
        String propertyName=updaterEntry.getKey();
        PropertyUpdater updater=updaterEntry.getValue();
        Map<String,String> typeMap=properties.get(type);
        if (typeMap != null && typeMap.containsKey(propertyName)) {
          typeMap.put(propertyName,updater.updateForClusterCreate(hostGroups,propertyName,typeMap.get(propertyName),properties,stackDefinition));
        }
      }
    }
  }
  if (isNameNodeHAEnabled()) {
    if (!isNameNodeHAInitialActiveNodeSet(properties) && !isNameNodeHAInitialStandbyNodeSet(properties)) {
      Collection<HostGroup> listOfHostGroups=new LinkedList<HostGroup>();
      for (      String key : hostGroups.keySet()) {
        listOfHostGroups.add(hostGroups.get(key));
      }
      Collection<HostGroup> hostGroupsContainingNameNode=getHostGroupsForComponent("NAMENODE",listOfHostGroups);
      Map<String,String> hadoopEnv=properties.get("hadoop-env");
      if (hostGroupsContainingNameNode.size() == 2) {
        List<HostGroup> listOfGroups=new LinkedList<HostGroup>(hostGroupsContainingNameNode);
        hadoopEnv.put("dfs_ha_initial_namenode_active",listOfGroups.get(0).getHostInfo().iterator().next());
        hadoopEnv.put("dfs_ha_initial_namenode_standby",listOfGroups.get(1).getHostInfo().iterator().next());
      }
 else {
        if (hostGroupsContainingNameNode.size() == 1) {
          List<String> listOfInfo=new LinkedList<String>(hostGroupsContainingNameNode.iterator().next().getHostInfo());
          hadoopEnv.put("dfs_ha_initial_namenode_active",listOfInfo.get(0));
          hadoopEnv.put("dfs_ha_initial_namenode_standby",listOfInfo.get(1));
        }
      }
    }
  }
  return properties;
}
