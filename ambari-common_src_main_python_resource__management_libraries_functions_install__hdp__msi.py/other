'\nLicensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n"License"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an "AS IS" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\nAmbari Agent\n\n'
from ambari_commons import os_utils
from ambari_commons.inet_utils import download_file
from ambari_commons.os_windows import SystemWideLock
from resource_management.core.resources.system import Execute
from resource_management.core.resources.system import File
from resource_management.core.logger import Logger
from resource_management.core.exceptions import Fail
from resource_management.libraries.functions.reload_windows_env import reload_windows_env
from resource_management.libraries.functions.windows_service_utils import check_windows_service_exists
from resource_management.libraries.functions.version import format_hdp_stack_version, compare_versions
import socket
import os
import glob
__all__ = ['install_windows_msi']
msi_save_dir = None
hdp_log_dir = 'c:\\hadoop\\logs'
hdp_data_dir = 'c:\\hadoopDefaultData'
local_host = socket.getfqdn()
db_flavor = 'DERBY'
hdp_22 = '#Namenode Data directory\nHDFS_NAMENODE_DATA_DIR={hdp_data_dir}\\hdpdatann\n\n#Datanode Data directory\nHDFS_DATANODE_DATA_DIR={hdp_data_dir}\\hdpdatadn\n'
cluster_properties = '#Log directory\nHDP_LOG_DIR={hdp_log_dir}\n\n#Data directory\nHDP_DATA_DIR={hdp_data_dir}\n\n{hdp_22_specific_props}\n\n#hosts\nNAMENODE_HOST={local_host}\nSECONDARY_NAMENODE_HOST={local_host}\nRESOURCEMANAGER_HOST={local_host}\nHIVE_SERVER_HOST={local_host}\nOOZIE_SERVER_HOST={local_host}\nWEBHCAT_HOST={local_host}\nSLAVE_HOSTS={local_host}\nZOOKEEPER_HOSTS={local_host}\nCLIENT_HOSTS={local_host}\nHBASE_MASTER={local_host}\nHBASE_REGIONSERVERS={local_host}\nFLUME_HOSTS={local_host}\nFALCON_HOST={local_host}\nKNOX_HOST={local_host}\nSTORM_NIMBUS={local_host}\nSTORM_SUPERVISORS={local_host}\n\n#Database host\nDB_FLAVOR={db_flavor}\nDB_HOSTNAME={local_host}\nDB_PORT=1527\n\n#Hive properties\nHIVE_DB_NAME=hive\nHIVE_DB_USERNAME=hive\nHIVE_DB_PASSWORD=hive\n\n#Oozie properties\nOOZIE_DB_NAME=oozie\nOOZIE_DB_USERNAME=oozie\nOOZIE_DB_PASSWORD=oozie\n'
INSTALL_MSI_CMD = 'cmd /C start /wait msiexec /qn /i  {hdp_msi_path} /lv {hdp_log_path} MSIUSEREALADMINDETECTION=1 HDP_LAYOUT={hdp_layout_path} DESTROY_DATA=yes HDP_USER_PASSWORD={hadoop_password_arg} HDP=yes KNOX=yes KNOX_MASTER_SECRET="AmbariHDP2Windows" FALCON=yes STORM=yes HBase=yes STORM=yes FLUME=yes RANGER=no'
CREATE_SERVICE_SCRIPT = os.path.abspath('sbin\\createservice.ps1')
CREATE_SERVICE_CMD = 'cmd /C powershell -File "{script}" -username hadoop -password "{password}" -servicename {servicename} -hdpresourcesdir "{resourcedir}" -servicecmdpath "{servicecmd}"'
INSTALL_MARKER_OK = 'msi.installed'
INSTALL_MARKER_FAILED = 'msi.failed'
_working_dir = None
