@retry(times=24, sleep_time=5, err_class=Fail)
def _check_datanode_shutdown(hdfs_binary):
    '\n  Checks that a DataNode is down by running "hdfs dfsamin getDatanodeInfo"\n  several times, pausing in between runs. Once the DataNode stops responding\n  this method will return, otherwise it will raise a Fail(...) and retry\n  automatically.\n  The stack defaults for retrying for HDFS are also way too slow for this\n  command; they are set to wait about 45 seconds between client retries. As\n  a result, a single execution of dfsadmin will take 45 seconds to retry and\n  the DataNode may be marked as dead, causing problems with HBase.\n  https://issues.apache.org/jira/browse/HDFS-8510 tracks reducing the\n  times for ipc.client.connect.retry.interval. In the meantime, override them\n  here, but only for RU.\n  :param hdfs_binary: name/path of the HDFS binary to use\n  :return:\n  '
    import params
    command = format('{hdfs_binary} dfsadmin -D ipc.client.connect.max.retries=5 -D ipc.client.connect.retry.interval=1000 -getDatanodeInfo {dfs_dn_ipc_address}')
    try:
        Execute(command, user=params.hdfs_user, tries=1)
    except:
        Logger.info('DataNode has successfully shutdown for upgrade.')
        return
    Logger.info('DataNode has not shutdown.')
    raise Fail('DataNode has not shutdown.')
