def setup_spark(env):
    import params
    env.set_params(params)
    Directory([params.spark_pid_dir, params.spark_log_dir], owner=params.spark_user, group=params.user_group, recursive=True)
    file_path = (params.spark_conf + '/spark-defaults.conf')
    create_file(file_path)
    write_properties_to_file(file_path, spark_properties(params))
    File(os.path.join(params.spark_conf, 'spark-env.sh'), owner=params.spark_user, group=params.spark_group, content=InlineTemplate(params.spark_env_sh))
    File(os.path.join(params.spark_conf, 'log4j.properties'), owner=params.spark_user, group=params.spark_group, content=params.spark_log4j_properties)
    File(os.path.join(params.spark_conf, 'metrics.properties'), owner=params.spark_user, group=params.spark_group, content=InlineTemplate(params.spark_metrics_properties))
    File(os.path.join(params.spark_conf, 'java-opts'), owner=params.spark_user, group=params.spark_group, content=params.spark_javaopts_properties)
    if params.is_hive_installed:
        hive_config = get_hive_config()
        XmlConfig('hive-site.xml', conf_dir=params.spark_conf, configurations=hive_config, owner=params.spark_user, group=params.spark_group, mode=420)
