def setup_spark(env, type, action=None):
    import params
    env.set_params(params)
    Directory([params.spark_pid_dir, params.spark_log_dir], owner=params.spark_user, group=params.user_group, recursive=True)
    if (type == 'server'):
        if ((action == 'start') or (action == 'config')):
            params.HdfsDirectory(params.spark_hdfs_user_dir, action='create', owner=params.spark_user, mode=509)
    file_path = (params.spark_conf + '/spark-defaults.conf')
    create_file(file_path)
    write_properties_to_file(file_path, spark_properties(params))
    File(os.path.join(params.spark_conf, 'spark-env.sh'), owner=params.spark_user, group=params.spark_group, content=InlineTemplate(params.spark_env_sh))
    File(os.path.join(params.spark_conf, 'log4j.properties'), owner=params.spark_user, group=params.spark_group, content=params.spark_log4j_properties)
    File(os.path.join(params.spark_conf, 'metrics.properties'), owner=params.spark_user, group=params.spark_group, content=InlineTemplate(params.spark_metrics_properties))
    File(os.path.join(params.spark_conf, 'java-opts'), owner=params.spark_user, group=params.spark_group, content=params.spark_javaopts_properties)
    if params.is_hive_installed:
        hive_config = get_hive_config()
        XmlConfig('hive-site.xml', conf_dir=params.spark_conf, configurations=hive_config, owner=params.spark_user, group=params.spark_group, mode=420)
