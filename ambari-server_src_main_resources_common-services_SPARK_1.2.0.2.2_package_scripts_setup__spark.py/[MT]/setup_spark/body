def setup_spark(env, type, action=None):
    import params
    Directory([params.spark_pid_dir, params.spark_log_dir], owner=params.spark_user, group=params.user_group, recursive=True)
    if ((type == 'server') and (action == 'config')):
        params.HdfsResource(params.spark_hdfs_user_dir, type='directory', action='create_on_execute', owner=params.spark_user, mode=509)
        params.HdfsResource(None, action='execute')
    PropertiesFile(format('{spark_conf}/spark-defaults.conf'), properties=params.config['configurations']['spark-defaults'], key_value_delimiter=' ')
    File(os.path.join(params.spark_conf, 'spark-env.sh'), owner=params.spark_user, group=params.spark_group, content=InlineTemplate(params.spark_env_sh))
    File(os.path.join(params.spark_conf, 'log4j.properties'), owner=params.spark_user, group=params.spark_group, content=params.spark_log4j_properties)
    File(os.path.join(params.spark_conf, 'metrics.properties'), owner=params.spark_user, group=params.spark_group, content=InlineTemplate(params.spark_metrics_properties))
    File(os.path.join(params.spark_conf, 'java-opts'), owner=params.spark_user, group=params.spark_group, content=params.spark_javaopts_properties)
    if params.is_hive_installed:
        XmlConfig('hive-site.xml', conf_dir=params.spark_conf, configurations=params.spark_hive_properties, owner=params.spark_user, group=params.spark_group, mode=420)
