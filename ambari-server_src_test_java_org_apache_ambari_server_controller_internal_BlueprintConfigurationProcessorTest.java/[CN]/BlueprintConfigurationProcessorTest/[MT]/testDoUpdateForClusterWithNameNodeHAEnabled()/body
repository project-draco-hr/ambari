{
  final String expectedNameService="mynameservice";
  final String expectedHostName="c6401.apache.ambari.org";
  final String expectedHostNameTwo="server-two";
  final String expectedPortNum="808080";
  final String expectedNodeOne="nn1";
  final String expectedNodeTwo="nn2";
  final String expectedHostGroupName="host_group_1";
  Map<String,Map<String,String>> properties=new HashMap<String,Map<String,String>>();
  Map<String,String> hdfsSiteProperties=new HashMap<String,String>();
  Map<String,String> hbaseSiteProperties=new HashMap<String,String>();
  Map<String,String> hadoopEnvProperties=new HashMap<String,String>();
  Map<String,String> coreSiteProperties=new HashMap<String,String>();
  Map<String,String> accumuloSiteProperties=new HashMap<String,String>();
  properties.put("hdfs-site",hdfsSiteProperties);
  properties.put("hadoop-env",hadoopEnvProperties);
  properties.put("core-site",coreSiteProperties);
  properties.put("hbase-site",hbaseSiteProperties);
  properties.put("accumulo-site",accumuloSiteProperties);
  hdfsSiteProperties.put("dfs.nameservices",expectedNameService);
  hdfsSiteProperties.put("dfs.ha.namenodes.mynameservice",expectedNodeOne + ", " + expectedNodeTwo);
  hdfsSiteProperties.put("dfs.namenode.https-address." + expectedNameService + "."+ expectedNodeOne,createExportedAddress(expectedPortNum,expectedHostGroupName));
  hdfsSiteProperties.put("dfs.namenode.https-address." + expectedNameService + "."+ expectedNodeTwo,createExportedAddress(expectedPortNum,expectedHostGroupName));
  hdfsSiteProperties.put("dfs.namenode.http-address." + expectedNameService + "."+ expectedNodeOne,createExportedAddress(expectedPortNum,expectedHostGroupName));
  hdfsSiteProperties.put("dfs.namenode.http-address." + expectedNameService + "."+ expectedNodeTwo,createExportedAddress(expectedPortNum,expectedHostGroupName));
  hdfsSiteProperties.put("dfs.namenode.rpc-address." + expectedNameService + "."+ expectedNodeOne,createExportedAddress(expectedPortNum,expectedHostGroupName));
  hdfsSiteProperties.put("dfs.namenode.rpc-address." + expectedNameService + "."+ expectedNodeTwo,createExportedAddress(expectedPortNum,expectedHostGroupName));
  hdfsSiteProperties.put("dfs.secondary.http.address","localhost:8080");
  hdfsSiteProperties.put("dfs.namenode.secondary.http-address","localhost:8080");
  hdfsSiteProperties.put("dfs.namenode.http-address","localhost:8080");
  hdfsSiteProperties.put("dfs.namenode.https-address","localhost:8081");
  hdfsSiteProperties.put("dfs.namenode.rpc-address","localhost:8082");
  coreSiteProperties.put("fs.defaultFS","hdfs://" + expectedNameService);
  hbaseSiteProperties.put("hbase.rootdir","hdfs://" + expectedNameService + "/hbase/test/root/dir");
  accumuloSiteProperties.put("instance.volumes","hdfs://" + expectedNameService + "/accumulo/test/instance/volumes");
  Configuration clusterConfig=new Configuration(properties,Collections.<String,Map<String,Map<String,String>>>emptyMap());
  Collection<String> hgComponents=new HashSet<String>();
  hgComponents.add("NAMENODE");
  TestHostGroup group1=new TestHostGroup(expectedHostGroupName,hgComponents,Collections.singleton(expectedHostName));
  Collection<String> hgComponents2=new HashSet<String>();
  hgComponents2.add("NAMENODE");
  TestHostGroup group2=new TestHostGroup("host-group-2",hgComponents2,Collections.singleton(expectedHostNameTwo));
  Collection<TestHostGroup> hostGroups=new ArrayList<TestHostGroup>();
  hostGroups.add(group1);
  hostGroups.add(group2);
  expect(stack.getCardinality("NAMENODE")).andReturn(new Cardinality("1-2")).anyTimes();
  expect(stack.getCardinality("SECONDARY_NAMENODE")).andReturn(new Cardinality("1")).anyTimes();
  ClusterTopology topology=createClusterTopology(bp,clusterConfig,hostGroups);
  BlueprintConfigurationProcessor updater=new BlueprintConfigurationProcessor(topology);
  Set<String> updatedConfigTypes=updater.doUpdateForClusterCreate();
  assertEquals("HTTPS address HA property not properly exported",expectedHostName + ":" + expectedPortNum,hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameService + "."+ expectedNodeOne));
  assertEquals("HTTPS address HA property not properly exported",expectedHostName + ":" + expectedPortNum,hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameService + "."+ expectedNodeTwo));
  assertEquals("HTTPS address HA property not properly exported",expectedHostName + ":" + expectedPortNum,hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameService + "."+ expectedNodeOne));
  assertEquals("HTTPS address HA property not properly exported",expectedHostName + ":" + expectedPortNum,hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameService + "."+ expectedNodeTwo));
  assertEquals("HTTPS address HA property not properly exported",expectedHostName + ":" + expectedPortNum,hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameService + "."+ expectedNodeOne));
  assertEquals("HTTPS address HA property not properly exported",expectedHostName + ":" + expectedPortNum,hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameService + "."+ expectedNodeTwo));
  String initialActiveHost=hadoopEnvProperties.get("dfs_ha_initial_namenode_active");
  String expectedStandbyHost=null;
  if (initialActiveHost.equals(expectedHostName)) {
    expectedStandbyHost=expectedHostNameTwo;
  }
 else   if (initialActiveHost.equals(expectedHostNameTwo)) {
    expectedStandbyHost=expectedHostName;
  }
 else {
    fail("Active Namenode hostname was not set correctly");
  }
  assertEquals("Standby Namenode hostname was not set correctly",expectedStandbyHost,hadoopEnvProperties.get("dfs_ha_initial_namenode_standby"));
  assertEquals("fs.defaultFS should not be modified by cluster update when NameNode HA is enabled.","hdfs://" + expectedNameService,coreSiteProperties.get("fs.defaultFS"));
  assertEquals("hbase.rootdir should not be modified by cluster update when NameNode HA is enabled.","hdfs://" + expectedNameService + "/hbase/test/root/dir",hbaseSiteProperties.get("hbase.rootdir"));
  assertEquals("instance.volumes should not be modified by cluster update when NameNode HA is enabled.","hdfs://" + expectedNameService + "/accumulo/test/instance/volumes",accumuloSiteProperties.get("instance.volumes"));
  assertFalse("dfs.namenode.http-address should have been filtered out of this HA configuration",hdfsSiteProperties.containsKey("dfs.namenode.http-address"));
  assertFalse("dfs.namenode.https-address should have been filtered out of this HA configuration",hdfsSiteProperties.containsKey("dfs.namenode.https-address"));
  assertFalse("dfs.namenode.rpc-address should have been filtered out of this HA configuration",hdfsSiteProperties.containsKey("dfs.namenode.rpc-address"));
  assertEquals("Incorrect number of updated config types returned, set = " + updatedConfigTypes,3,updatedConfigTypes.size());
  assertTrue("Expected config type not found in updated set",updatedConfigTypes.contains("cluster-env"));
  assertTrue("Expected config type not found in updated set",updatedConfigTypes.contains("hdfs-site"));
  assertTrue("Expected config type not found in updated set",updatedConfigTypes.contains("hadoop-env"));
}
