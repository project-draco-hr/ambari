{
  final String expectedHostGroupName="host_group_1";
  Map<String,Map<String,String>> properties=new HashMap<String,Map<String,String>>();
  Map<String,String> stormSiteProperties=new HashMap<String,String>();
  Map<String,String> kafkaBrokerProperties=new HashMap<String,String>();
  properties.put("storm-site",stormSiteProperties);
  properties.put("kafka-broker",kafkaBrokerProperties);
  stormSiteProperties.put("worker.childopts","localhost");
  stormSiteProperties.put("supervisor.childopts","localhost");
  stormSiteProperties.put("nimbus.childopts","localhost");
  kafkaBrokerProperties.put("kafka.ganglia.metrics.host","localhost");
  Configuration clusterConfig=new Configuration(properties,Collections.<String,Map<String,Map<String,String>>>emptyMap());
  Collection<String> hgComponents=new HashSet<String>();
  hgComponents.add("HIVE_METASTORE");
  TestHostGroup group1=new TestHostGroup(expectedHostGroupName,hgComponents,Collections.singleton("testserver"));
  Collection<TestHostGroup> hostGroups=new HashSet<TestHostGroup>();
  hostGroups.add(group1);
  expect(stack.getCardinality("GANGLIA_SERVER")).andReturn(new Cardinality("1")).anyTimes();
  ClusterTopology topology=createClusterTopology(bp,clusterConfig,hostGroups);
  BlueprintConfigurationProcessor updater=new BlueprintConfigurationProcessor(topology);
  updater.doUpdateForClusterCreate();
  assertEquals("worker startup settings not properly handled by cluster create","localhost",stormSiteProperties.get("worker.childopts"));
  assertEquals("supervisor startup settings not properly handled by cluster create","localhost",stormSiteProperties.get("supervisor.childopts"));
  assertEquals("nimbus startup settings not properly handled by cluster create","localhost",stormSiteProperties.get("nimbus.childopts"));
  assertEquals("Kafka ganglia host property not properly handled by cluster create","localhost",kafkaBrokerProperties.get("kafka.ganglia.metrics.host"));
}
