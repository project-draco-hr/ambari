{
  final String expectedNameService="mynameservice";
  final String expectedHostName="c6401.apache.ambari.org";
  final String expectedHostNameTwo="serverTwo";
  final String expectedPortNum="808080";
  final String expectedNodeOne="nn1";
  final String expectedNodeTwo="nn2";
  final String expectedHostGroupName="host_group_1";
  EasyMockSupport mockSupport=new EasyMockSupport();
  HostGroup mockHostGroupOne=mockSupport.createMock(HostGroup.class);
  HostGroup mockHostGroupTwo=mockSupport.createMock(HostGroup.class);
  Stack mockStack=mockSupport.createMock(Stack.class);
  expect(mockHostGroupOne.getHostInfo()).andReturn(Arrays.asList(expectedHostName)).atLeastOnce();
  expect(mockHostGroupTwo.getHostInfo()).andReturn(Arrays.asList(expectedHostNameTwo)).atLeastOnce();
  expect(mockHostGroupOne.getComponents()).andReturn(Collections.singleton("NAMENODE")).atLeastOnce();
  expect(mockHostGroupTwo.getComponents()).andReturn(Collections.singleton("NAMENODE")).atLeastOnce();
  expect(mockStack.getCardinality("NAMENODE")).andReturn(new Cardinality("1-2")).atLeastOnce();
  expect(mockStack.getCardinality("SECONDARY_NAMENODE")).andReturn(new Cardinality("1")).atLeastOnce();
  mockSupport.replayAll();
  Map<String,Map<String,String>> configProperties=new HashMap<String,Map<String,String>>();
  Map<String,String> hdfsSiteProperties=new HashMap<String,String>();
  Map<String,String> hbaseSiteProperties=new HashMap<String,String>();
  Map<String,String> hadoopEnvProperties=new HashMap<String,String>();
  Map<String,String> coreSiteProperties=new HashMap<String,String>();
  Map<String,String> accumuloSiteProperties=new HashMap<String,String>();
  configProperties.put("hdfs-site",hdfsSiteProperties);
  configProperties.put("hadoop-env",hadoopEnvProperties);
  configProperties.put("core-site",coreSiteProperties);
  configProperties.put("hbase-site",hbaseSiteProperties);
  configProperties.put("accumulo-site",accumuloSiteProperties);
  hdfsSiteProperties.put("dfs.nameservices",expectedNameService);
  hdfsSiteProperties.put("dfs.ha.namenodes.mynameservice",expectedNodeOne + ", " + expectedNodeTwo);
  hdfsSiteProperties.put("dfs.namenode.https-address." + expectedNameService + "."+ expectedNodeOne,createHostAddress(expectedHostName,expectedPortNum));
  hdfsSiteProperties.put("dfs.namenode.https-address." + expectedNameService + "."+ expectedNodeTwo,createHostAddress(expectedHostNameTwo,expectedPortNum));
  hdfsSiteProperties.put("dfs.namenode.http-address." + expectedNameService + "."+ expectedNodeOne,createHostAddress(expectedHostName,expectedPortNum));
  hdfsSiteProperties.put("dfs.namenode.http-address." + expectedNameService + "."+ expectedNodeTwo,createHostAddress(expectedHostNameTwo,expectedPortNum));
  hdfsSiteProperties.put("dfs.namenode.rpc-address." + expectedNameService + "."+ expectedNodeOne,createHostAddress(expectedHostName,expectedPortNum));
  hdfsSiteProperties.put("dfs.namenode.rpc-address." + expectedNameService + "."+ expectedNodeTwo,createHostAddress(expectedHostNameTwo,expectedPortNum));
  hdfsSiteProperties.put("dfs.secondary.http.address","localhost:8080");
  hdfsSiteProperties.put("dfs.namenode.secondary.http-address","localhost:8080");
  coreSiteProperties.put("fs.defaultFS","hdfs://" + expectedNameService);
  hbaseSiteProperties.put("hbase.rootdir","hdfs://" + expectedNameService + "/hbase/test/root/dir");
  accumuloSiteProperties.put("instance.volumes","hdfs://" + expectedNameService + "/accumulo/test/instance/volumes");
  BlueprintConfigurationProcessor configProcessor=new BlueprintConfigurationProcessor(configProperties);
  Map<String,HostGroup> mapOfHostGroups=new LinkedHashMap<String,HostGroup>();
  mapOfHostGroups.put(expectedHostGroupName,mockHostGroupOne);
  mapOfHostGroups.put("host-group-2",mockHostGroupTwo);
  configProcessor.doUpdateForClusterCreate(mapOfHostGroups,mockStack);
  assertEquals("HTTPS address HA property not properly exported",expectedHostName + ":" + expectedPortNum,hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameService + "."+ expectedNodeOne));
  assertEquals("HTTPS address HA property not properly exported",expectedHostNameTwo + ":" + expectedPortNum,hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameService + "."+ expectedNodeTwo));
  assertEquals("HTTPS address HA property not properly exported",expectedHostName + ":" + expectedPortNum,hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameService + "."+ expectedNodeOne));
  assertEquals("HTTPS address HA property not properly exported",expectedHostNameTwo + ":" + expectedPortNum,hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameService + "."+ expectedNodeTwo));
  assertEquals("HTTPS address HA property not properly exported",expectedHostName + ":" + expectedPortNum,hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameService + "."+ expectedNodeOne));
  assertEquals("HTTPS address HA property not properly exported",expectedHostNameTwo + ":" + expectedPortNum,hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameService + "."+ expectedNodeTwo));
  assertEquals("Active Namenode hostname was not set correctly",expectedHostName,hadoopEnvProperties.get("dfs_ha_initial_namenode_active"));
  assertEquals("Standby Namenode hostname was not set correctly",expectedHostNameTwo,hadoopEnvProperties.get("dfs_ha_initial_namenode_standby"));
  assertEquals("fs.defaultFS should not be modified by cluster update when NameNode HA is enabled.","hdfs://" + expectedNameService,coreSiteProperties.get("fs.defaultFS"));
  assertEquals("hbase.rootdir should not be modified by cluster update when NameNode HA is enabled.","hdfs://" + expectedNameService + "/hbase/test/root/dir",hbaseSiteProperties.get("hbase.rootdir"));
  assertEquals("instance.volumes should not be modified by cluster update when NameNode HA is enabled.","hdfs://" + expectedNameService + "/accumulo/test/instance/volumes",accumuloSiteProperties.get("instance.volumes"));
  mockSupport.verifyAll();
}
