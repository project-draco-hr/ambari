{
  final String expectedNameService="mynameservice";
  final String expectedHostName="c6401.apache.ambari.org";
  final String expectedHostNameTwo="server-two";
  final String expectedPortNum="808080";
  final String expectedNodeOne="nn1";
  final String expectedNodeTwo="nn2";
  final String expectedHostGroupName="host_group_1";
  Map<String,Map<String,String>> configProperties=new HashMap<String,Map<String,String>>();
  Map<String,String> hdfsSiteProperties=new HashMap<String,String>();
  Map<String,String> hbaseSiteProperties=new HashMap<String,String>();
  Map<String,String> hadoopEnvProperties=new HashMap<String,String>();
  Map<String,String> coreSiteProperties=new HashMap<String,String>();
  Map<String,String> accumuloSiteProperties=new HashMap<String,String>();
  configProperties.put("hdfs-site",hdfsSiteProperties);
  configProperties.put("hadoop-env",hadoopEnvProperties);
  configProperties.put("core-site",coreSiteProperties);
  configProperties.put("hbase-site",hbaseSiteProperties);
  configProperties.put("accumulo-site",accumuloSiteProperties);
  hdfsSiteProperties.put("dfs.nameservices",expectedNameService);
  hdfsSiteProperties.put("dfs.ha.namenodes.mynameservice",expectedNodeOne + ", " + expectedNodeTwo);
  hdfsSiteProperties.put("dfs.namenode.https-address." + expectedNameService + "."+ expectedNodeOne,createHostAddress(expectedHostName,expectedPortNum));
  hdfsSiteProperties.put("dfs.namenode.https-address." + expectedNameService + "."+ expectedNodeTwo,createHostAddress(expectedHostNameTwo,expectedPortNum));
  hdfsSiteProperties.put("dfs.namenode.http-address." + expectedNameService + "."+ expectedNodeOne,createHostAddress(expectedHostName,expectedPortNum));
  hdfsSiteProperties.put("dfs.namenode.http-address." + expectedNameService + "."+ expectedNodeTwo,createHostAddress(expectedHostNameTwo,expectedPortNum));
  hdfsSiteProperties.put("dfs.namenode.rpc-address." + expectedNameService + "."+ expectedNodeOne,createHostAddress(expectedHostName,expectedPortNum));
  hdfsSiteProperties.put("dfs.namenode.rpc-address." + expectedNameService + "."+ expectedNodeTwo,createHostAddress(expectedHostNameTwo,expectedPortNum));
  hdfsSiteProperties.put("dfs.secondary.http.address","localhost:8080");
  hdfsSiteProperties.put("dfs.namenode.secondary.http-address","localhost:8080");
  coreSiteProperties.put("fs.defaultFS","hdfs://" + expectedNameService);
  hbaseSiteProperties.put("hbase.rootdir","hdfs://" + expectedNameService + "/hbase/test/root/dir");
  accumuloSiteProperties.put("instance.volumes","hdfs://" + expectedNameService + "/accumulo/test/instance/volumes");
  Configuration clusterConfig=new Configuration(configProperties,Collections.<String,Map<String,Map<String,String>>>emptyMap());
  Collection<String> hgComponents=new HashSet<String>();
  hgComponents.add("NAMENODE");
  TestHostGroup group1=new TestHostGroup(expectedHostGroupName,hgComponents,Collections.singleton(expectedHostName));
  Collection<String> hgComponents2=new HashSet<String>();
  hgComponents2.add("NAMENODE");
  TestHostGroup group2=new TestHostGroup("host-group-2",hgComponents2,Collections.singleton(expectedHostNameTwo));
  Collection<TestHostGroup> hostGroups=new HashSet<TestHostGroup>();
  hostGroups.add(group1);
  hostGroups.add(group2);
  expect(stack.getCardinality("NAMENODE")).andReturn(new Cardinality("1-2")).anyTimes();
  expect(stack.getCardinality("SECONDARY_NAMENODE")).andReturn(new Cardinality("1")).anyTimes();
  ClusterTopology topology=createClusterTopology(bp,clusterConfig,hostGroups);
  BlueprintConfigurationProcessor updater=new BlueprintConfigurationProcessor(topology);
  updater.doUpdateForClusterCreate();
  assertEquals("HTTPS address HA property not properly exported",expectedHostName + ":" + expectedPortNum,hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameService + "."+ expectedNodeOne));
  assertEquals("HTTPS address HA property not properly exported",expectedHostNameTwo + ":" + expectedPortNum,hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameService + "."+ expectedNodeTwo));
  assertEquals("HTTPS address HA property not properly exported",expectedHostName + ":" + expectedPortNum,hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameService + "."+ expectedNodeOne));
  assertEquals("HTTPS address HA property not properly exported",expectedHostNameTwo + ":" + expectedPortNum,hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameService + "."+ expectedNodeTwo));
  assertEquals("HTTPS address HA property not properly exported",expectedHostName + ":" + expectedPortNum,hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameService + "."+ expectedNodeOne));
  assertEquals("HTTPS address HA property not properly exported",expectedHostNameTwo + ":" + expectedPortNum,hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameService + "."+ expectedNodeTwo));
  String activeHost=hadoopEnvProperties.get("dfs_ha_initial_namenode_active");
  if (activeHost.equals(expectedHostName)) {
    assertEquals("Standby Namenode hostname was not set correctly",expectedHostNameTwo,hadoopEnvProperties.get("dfs_ha_initial_namenode_standby"));
  }
 else   if (activeHost.equals(expectedHostNameTwo)) {
    assertEquals("Standby Namenode hostname was not set correctly",expectedHostName,hadoopEnvProperties.get("dfs_ha_initial_namenode_standby"));
  }
 else {
    fail("Active Namenode hostname was not set correctly: " + activeHost);
  }
  assertEquals("fs.defaultFS should not be modified by cluster update when NameNode HA is enabled.","hdfs://" + expectedNameService,coreSiteProperties.get("fs.defaultFS"));
  assertEquals("hbase.rootdir should not be modified by cluster update when NameNode HA is enabled.","hdfs://" + expectedNameService + "/hbase/test/root/dir",hbaseSiteProperties.get("hbase.rootdir"));
  assertEquals("instance.volumes should not be modified by cluster update when NameNode HA is enabled.","hdfs://" + expectedNameService + "/accumulo/test/instance/volumes",accumuloSiteProperties.get("instance.volumes"));
}
