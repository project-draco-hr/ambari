def copy_tarballs_to_hdfs(source, dest, hdp_select_component_name, component_user, file_owner, group_owner):
    '\n  :param tarball_prefix: Prefix of the tarball must be one of tez, hive, mr, pig\n  :param hdp_select_component_name: Component name to get the status to determine the version\n  :param component_user: User that will execute the Hadoop commands\n  :param file_owner: Owner of the files copied to HDFS (typically hdfs account)\n  :param group_owner: Group owner of the files copied to HDFS (typically hadoop group)\n  :return: Returns 0 on success, 1 if no files were copied, and in some cases may raise an exception.\n \n  In order to call this function, params.py must have all of the following,\n  hdp_stack_version, kinit_path_local, security_enabled, hdfs_user, hdfs_principal_name, hdfs_user_keytab,\n  hadoop_bin_dir, hadoop_conf_dir, and HdfsDirectory as a partial function.\n  '
    (component_tar_source_file, component_tar_destination_folder) = (source, dest)
    if (not os.path.exists(component_tar_source_file)):
        Logger.warning(('Could not find file: %s' % str(component_tar_source_file)))
        return 1
    tmpfile = tempfile.NamedTemporaryFile()
    out = None
    with open(tmpfile.name, 'r+') as file:
        get_hdp_version_cmd = ('/usr/bin/hdp-select status %s > %s' % (hdp_select_component_name, tmpfile.name))
        (code, stdoutdata) = shell.call(get_hdp_version_cmd)
        out = file.read()
    pass
    if ((code != 0) or (out is None)):
        Logger.warning(("Could not verify HDP version by calling '%s'. Return Code: %s, Output: %s." % (get_hdp_version_cmd, str(code), str(out))))
        return 1
    matches = re.findall('([\\d\\.]+\\-\\d+)', out)
    hdp_version = (matches[0] if (matches and (len(matches) > 0)) else None)
    if (not hdp_version):
        Logger.error(('Could not parse HDP version from output of hdp-select: %s' % str(out)))
        return 1
    file_name = os.path.basename(component_tar_source_file)
    destination_file = os.path.join(component_tar_destination_folder, file_name)
    destination_file = destination_file.replace('{{ hdp_stack_version }}', hdp_version)
    does_hdfs_file_exist_cmd = ('fs -ls %s' % destination_file)
    kinit_if_needed = ''
    if params.security_enabled:
        kinit_if_needed = format('{kinit_path_local} -kt {hdfs_user_keytab} {hdfs_principal_name};')
    if kinit_if_needed:
        Execute(kinit_if_needed, user=component_user, path='/bin')
    does_hdfs_file_exist = False
    try:
        ExecuteHadoop(does_hdfs_file_exist_cmd, user=component_user, logoutput=True, conf_dir=params.hadoop_conf_dir, bin_dir=params.hadoop_bin_dir)
        does_hdfs_file_exist = True
    except Fail:
        pass
    if (not does_hdfs_file_exist):
        source_and_dest_pairs = [(component_tar_source_file, destination_file)]
        return _copy_files(source_and_dest_pairs, file_owner, group_owner, kinit_if_needed)
    return 1
