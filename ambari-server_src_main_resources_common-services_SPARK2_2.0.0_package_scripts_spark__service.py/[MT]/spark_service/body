def spark_service(name, upgrade_type=None, action=None):
    import params
    if (action == 'start'):
        effective_version = (params.version if (upgrade_type is not None) else params.stack_version_formatted)
        if effective_version:
            effective_version = format_stack_version(effective_version)
        if ((name == 'jobhistoryserver') and effective_version and check_stack_feature(StackFeature.SPARK_16PLUS, effective_version)):
            source_dir = (params.spark_home + '/jars')
            tmp_archive_file = get_tarball_paths('spark2')[1]
            make_tarfile(tmp_archive_file, source_dir)
            copy_to_hdfs('spark2', params.user_group, params.hdfs_user, skip=params.sysprep_skip_copy_tarballs_hdfs)
            params.HdfsResource(params.spark_history_dir, type='directory', action='create_on_execute', owner=params.spark_user, group=params.user_group, mode=511, recursive_chmod=True)
            params.HdfsResource(None, action='execute')
        if params.security_enabled:
            spark_kinit_cmd = format('{kinit_path_local} -kt {spark_kerberos_keytab} {spark_principal}; ')
            Execute(spark_kinit_cmd, user=params.spark_user)
        if (params.stack_version_formatted and check_stack_feature(StackFeature.TEZ_FOR_SPARK, params.stack_version_formatted)):
            resource_created = copy_to_hdfs('tez', params.user_group, params.hdfs_user, skip=params.sysprep_skip_copy_tarballs_hdfs)
            if resource_created:
                params.HdfsResource(None, action='execute')
        if (name == 'jobhistoryserver'):
            historyserver_no_op_test = format('ls {spark_history_server_pid_file} >/dev/null 2>&1 && ps -p `cat {spark_history_server_pid_file}` >/dev/null 2>&1')
            try:
                Execute(format('{spark_history_server_start}'), user=params.spark_user, environment={'JAVA_HOME': params.java_home, }, not_if=historyserver_no_op_test)
            except:
                show_logs(params.spark_log_dir, user=params.spark_user)
                raise
        elif (name == 'sparkthriftserver'):
            if params.security_enabled:
                hive_principal = params.hive_kerberos_principal
                hive_kinit_cmd = format('{kinit_path_local} -kt {hive_kerberos_keytab} {hive_principal}; ')
                Execute(hive_kinit_cmd, user=params.hive_user)
            thriftserver_no_op_test = format('ls {spark_thrift_server_pid_file} >/dev/null 2>&1 && ps -p `cat {spark_thrift_server_pid_file}` >/dev/null 2>&1')
            try:
                Execute(format('{spark_thrift_server_start} --properties-file {spark_thrift_server_conf_file} {spark_thrift_cmd_opts_properties}'), user=params.hive_user, environment={'JAVA_HOME': params.java_home, }, not_if=thriftserver_no_op_test)
            except:
                show_logs(params.spark_log_dir, user=params.hive_user)
                raise
    elif (action == 'stop'):
        if (name == 'jobhistoryserver'):
            try:
                Execute(format('{spark_history_server_stop}'), user=params.spark_user, environment={'JAVA_HOME': params.java_home, })
            except:
                show_logs(params.spark_log_dir, user=params.spark_user)
                raise
            File(params.spark_history_server_pid_file, action='delete')
        elif (name == 'sparkthriftserver'):
            try:
                Execute(format('{spark_thrift_server_stop}'), user=params.hive_user, environment={'JAVA_HOME': params.java_home, })
            except:
                show_logs(params.spark_log_dir, user=params.hive_user)
                raise
            File(params.spark_thrift_server_pid_file, action='delete')
