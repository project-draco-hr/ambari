@patch.object(Script, 'is_stack_greater_or_equal', new=MagicMock(return_value=True))
@patch('resource_management.libraries.functions.copy_tarball.copy_to_hdfs')
def test_pre_upgrade_restart(self, copy_to_hdfs_mock):
    copy_to_hdfs_mock.return_value = True
    config_file = (self.get_src_folder() + '/test/python/stacks/2.0.6/configs/default.json')
    with open(config_file, 'r') as f:
        json_content = json.load(f)
    version = '2.2.1.0-3242'
    json_content['commandParams']['version'] = version
    self.executeScript((self.COMMON_SERVICES_PACKAGE_DIR + '/scripts/hive_server.py'), classname='HiveServer', command='pre_upgrade_restart', config_dict=json_content, stack_version=self.STACK_VERSION, target=RMFTestCase.TARGET_COMMON_SERVICES)
    self.assertResourceCalled('Execute', ('ambari-python-wrap', '/usr/bin/hdp-select', 'set', 'hive-server2', version), sudo=True)
    copy_to_hdfs_mock.assert_any_call('mapreduce', 'hadoop', 'hdfs', host_sys_prepped=False)
    copy_to_hdfs_mock.assert_any_call('tez', 'hadoop', 'hdfs', host_sys_prepped=False)
    self.assertEquals(2, copy_to_hdfs_mock.call_count)
    self.assertResourceCalled('HdfsResource', None, immutable_paths=self.DEFAULT_IMMUTABLE_PATHS, security_enabled=False, hadoop_bin_dir='/usr/hdp/current/hadoop-client/bin', keytab=UnknownConfigurationMock(), kinit_path_local='/usr/bin/kinit', user='hdfs', dfs_type='', action=['execute'], hdfs_resource_ignore_file='/var/lib/ambari-agent/data/.hdfs_resource_ignore', hdfs_site=self.getConfig()['configurations']['hdfs-site'], principal_name='missing_principal', default_fs='hdfs://c6401.ambari.apache.org:8020', hadoop_conf_dir='/usr/hdp/current/hadoop-client/conf')
    self.assertNoMoreResources()
