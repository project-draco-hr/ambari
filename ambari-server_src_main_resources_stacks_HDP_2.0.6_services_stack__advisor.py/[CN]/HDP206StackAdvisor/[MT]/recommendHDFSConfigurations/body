def recommendHDFSConfigurations(self, configurations, clusterData, services, hosts):
    putHDFSProperty = self.putProperty(configurations, 'hadoop-env', services)
    putHDFSSiteProperty = self.putProperty(configurations, 'hdfs-site', services)
    putHDFSSitePropertyAttributes = self.putPropertyAttribute(configurations, 'hdfs-site')
    putHDFSProperty('namenode_heapsize', max(int((clusterData['totalAvailableRam'] / 2)), 1024))
    putHDFSProperty = self.putProperty(configurations, 'hadoop-env', services)
    putHDFSProperty('namenode_opt_newsize', max(int((clusterData['totalAvailableRam'] / 8)), 128))
    putHDFSProperty = self.putProperty(configurations, 'hadoop-env', services)
    putHDFSProperty('namenode_opt_maxnewsize', max(int((clusterData['totalAvailableRam'] / 8)), 256))
    hdfsSiteProperties = getServicesSiteProperties(services, 'hdfs-site')
    nameServices = None
    if (hdfsSiteProperties and ('dfs.internal.nameservices' in hdfsSiteProperties)):
        nameServices = hdfsSiteProperties['dfs.internal.nameservices']
    if ((nameServices is None) and hdfsSiteProperties and ('dfs.nameservices' in hdfsSiteProperties)):
        nameServices = hdfsSiteProperties['dfs.nameservices']
    if (nameServices and (('dfs.ha.namenodes.%s' % nameServices) in hdfsSiteProperties)):
        namenodes = hdfsSiteProperties[('dfs.ha.namenodes.%s' % nameServices)]
        if (len(namenodes.split(',')) > 1):
            putHDFSSitePropertyAttributes('dfs.namenode.rpc-address', 'delete', 'true')
    if ((not hdfsSiteProperties) or ('dfs.datanode.data.dir' not in hdfsSiteProperties)):
        dataDirs = '/hadoop/hdfs/data'
        putHDFSSiteProperty('dfs.datanode.data.dir', dataDirs)
    else:
        dataDirs = hdfsSiteProperties['dfs.datanode.data.dir'].split(',')
    mountPoints = []
    mountPointDiskAvailableSpace = []
    for host in hosts['items']:
        for diskInfo in host['Hosts']['disk_info']:
            mountPoints.append(diskInfo['mountpoint'])
            mountPointDiskAvailableSpace.append(long(diskInfo['size']))
    maxFreeVolumeSize = 0L
    for dataDir in dataDirs:
        mp = getMountPointForDir(dataDir, mountPoints)
        for i in range(len(mountPoints)):
            if (mp == mountPoints[i]):
                if (mountPointDiskAvailableSpace[i] > maxFreeVolumeSize):
                    maxFreeVolumeSize = mountPointDiskAvailableSpace[i]
    putHDFSSiteProperty('dfs.datanode.du.reserved', ((maxFreeVolumeSize * 1024) / 8))
    self.recommendHadoopProxyUsers(configurations, services, hosts)
