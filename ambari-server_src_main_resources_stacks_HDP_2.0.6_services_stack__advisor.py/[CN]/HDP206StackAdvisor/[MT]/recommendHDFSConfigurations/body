def recommendHDFSConfigurations(self, configurations, clusterData, services, hosts):
    putHDFSProperty = self.putProperty(configurations, 'hadoop-env', services)
    putHDFSSiteProperty = self.putProperty(configurations, 'hdfs-site', services)
    putHDFSSitePropertyAttributes = self.putPropertyAttribute(configurations, 'hdfs-site')
    putHDFSProperty('namenode_heapsize', max(int((clusterData['totalAvailableRam'] / 2)), 1024))
    putHDFSProperty = self.putProperty(configurations, 'hadoop-env', services)
    putHDFSProperty('namenode_opt_newsize', max(int((clusterData['totalAvailableRam'] / 8)), 128))
    putHDFSProperty = self.putProperty(configurations, 'hadoop-env', services)
    putHDFSProperty('namenode_opt_maxnewsize', max(int((clusterData['totalAvailableRam'] / 8)), 256))
    hdfsSiteProperties = getServicesSiteProperties(services, 'hdfs-site')
    nameServices = None
    if (hdfsSiteProperties and ('dfs.internal.nameservices' in hdfsSiteProperties)):
        nameServices = hdfsSiteProperties['dfs.internal.nameservices']
    if ((nameServices is None) and hdfsSiteProperties and ('dfs.nameservices' in hdfsSiteProperties)):
        nameServices = hdfsSiteProperties['dfs.nameservices']
    if (nameServices and (('dfs.ha.namenodes.%s' % nameServices) in hdfsSiteProperties)):
        namenodes = hdfsSiteProperties[('dfs.ha.namenodes.%s' % nameServices)]
        if (len(namenodes.split(',')) > 1):
            putHDFSSitePropertyAttributes('dfs.namenode.rpc-address', 'delete', 'true')
    hdfs_mount_properties = [('dfs.datanode.data.dir', 'DATANODE', '/hadoop/hdfs/data', 'multi'), ('dfs.name.dir', 'NAMENODE', '/hadoop/hdfs/namenode', 'multi'), ('dfs.namenode.name.dir', 'DATANODE', '/hadoop/hdfs/namenode', 'multi'), ('dfs.data.dir', 'DATANODE', '/hadoop/hdfs/data', 'multi'), ('fs.checkpoint.dir', 'SECONDARY_NAMENODE', '/hadoop/hdfs/namesecondary', 'single'), ('dfs.namenode.checkpoint.dir', 'SECONDARY_NAMENODE', '/hadoop/hdfs/namesecondary', 'single')]
    self.updateMountProperties('hdfs-site', hdfs_mount_properties, configurations, services, hosts)
    dataDirs = hdfsSiteProperties['dfs.datanode.data.dir'].split(',')
    reservedSizeRecommendation = 0L
    for host in hosts['items']:
        mountPoints = []
        mountPointDiskAvailableSpace = []
        for diskInfo in host['Hosts']['disk_info']:
            mountPoints.append(diskInfo['mountpoint'])
            mountPointDiskAvailableSpace.append(long(diskInfo['size']))
        maxFreeVolumeSizeForHost = 0L
        for dataDir in dataDirs:
            mp = getMountPointForDir(dataDir, mountPoints)
            for i in range(len(mountPoints)):
                if (mp == mountPoints[i]):
                    if (mountPointDiskAvailableSpace[i] > maxFreeVolumeSizeForHost):
                        maxFreeVolumeSizeForHost = mountPointDiskAvailableSpace[i]
        if ((not reservedSizeRecommendation) or (maxFreeVolumeSizeForHost and (maxFreeVolumeSizeForHost < reservedSizeRecommendation))):
            reservedSizeRecommendation = maxFreeVolumeSizeForHost
    if reservedSizeRecommendation:
        reservedSizeRecommendation = max(((reservedSizeRecommendation * 1024) / 8), 1073741824)
        putHDFSSiteProperty('dfs.datanode.du.reserved', reservedSizeRecommendation)
    self.recommendHadoopProxyUsers(configurations, services, hosts)
