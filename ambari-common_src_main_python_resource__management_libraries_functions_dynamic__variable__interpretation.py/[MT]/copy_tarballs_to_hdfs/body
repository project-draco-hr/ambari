def copy_tarballs_to_hdfs(tarball_prefix, stack_select_component_name, component_user, file_owner, group_owner, ignore_sysprep=False):
    '\n  :param tarball_prefix: Prefix of the tarball must be one of tez, hive, mr, pig\n  :param stack_select_component_name: Component name to get the status to determine the version\n  :param component_user: User that will execute the Hadoop commands, usually smokeuser\n  :param file_owner: Owner of the files copied to HDFS (typically hdfs user)\n  :param group_owner: Group owner of the files copied to HDFS (typically hadoop group)\n  :param ignore_sysprep: Ignore sysprep directives\n  :return: Returns 0 on success, 1 if no files were copied, and in some cases may raise an exception.\n\n  In order to call this function, params.py must have all of the following,\n  stack_version_formatted, kinit_path_local, security_enabled, hdfs_user, hdfs_principal_name, hdfs_user_keytab,\n  hadoop_bin_dir, hadoop_conf_dir, and HdfsDirectory as a partial function.\n  '
    import params
    if ((not ignore_sysprep) and hasattr(params, 'host_sys_prepped') and params.host_sys_prepped):
        Logger.info(('Host is sys-prepped. Tarball %s will not be copied for %s.' % (tarball_prefix, stack_select_component_name)))
        return 0
    if ((not hasattr(params, 'stack_version_formatted')) or (params.stack_version_formatted is None)):
        Logger.warning('Could not find stack_version_formatted')
        return 1
    (component_tar_source_file, component_tar_destination_folder) = _get_tar_source_and_dest_folder(tarball_prefix)
    if ((not component_tar_source_file) or (not component_tar_destination_folder)):
        Logger.warning(('Could not retrieve properties for tarball with prefix: %s' % str(tarball_prefix)))
        return 1
    if (not os.path.exists(component_tar_source_file)):
        Logger.warning(('Could not find file: %s' % str(component_tar_source_file)))
        return 1
    tmpfile = tempfile.NamedTemporaryFile()
    out = None
    with open(tmpfile.name, 'r+') as file:
        get_stack_version_cmd = ('/usr/bin/hdp-select status %s > %s' % (stack_select_component_name, tmpfile.name))
        (code, stdoutdata) = shell.call(get_stack_version_cmd)
        out = file.read()
    pass
    if ((code != 0) or (out is None)):
        Logger.warning(("Could not verify HDP version by calling '%s'. Return Code: %s, Output: %s." % (get_stack_version_cmd, str(code), str(out))))
        return 1
    matches = re.findall('([\\d\\.]+\\-\\d+)', out)
    stack_version = (matches[0] if (matches and (len(matches) > 0)) else None)
    if (not stack_version):
        Logger.error(('Could not parse HDP version from output of hdp-select: %s' % str(out)))
        return 1
    file_name = os.path.basename(component_tar_source_file)
    destination_file = os.path.join(component_tar_destination_folder, file_name)
    destination_file = destination_file.replace('{{ stack_version_formatted }}', stack_version)
    does_hdfs_file_exist_cmd = ('fs -ls %s' % destination_file)
    kinit_if_needed = ''
    if params.security_enabled:
        kinit_if_needed = format('{kinit_path_local} -kt {hdfs_user_keytab} {hdfs_principal_name};')
    if kinit_if_needed:
        Execute(kinit_if_needed, user=component_user, path='/bin')
    does_hdfs_file_exist = False
    try:
        ExecuteHadoop(does_hdfs_file_exist_cmd, user=component_user, logoutput=True, conf_dir=params.hadoop_conf_dir, bin_dir=params.hadoop_bin_dir)
        does_hdfs_file_exist = True
    except Fail:
        pass
    if (not does_hdfs_file_exist):
        source_and_dest_pairs = [(component_tar_source_file, destination_file)]
        return _copy_files(source_and_dest_pairs, component_user, file_owner, group_owner, kinit_if_needed)
    return 1
