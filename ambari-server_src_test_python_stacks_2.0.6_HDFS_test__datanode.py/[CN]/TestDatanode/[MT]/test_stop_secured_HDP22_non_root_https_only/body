@patch('os.path.exists', new=MagicMock(return_value=False))
def test_stop_secured_HDP22_non_root_https_only(self):
    config_file = (self.get_src_folder() + '/test/python/stacks/2.0.6/configs/secured.json')
    with open(config_file, 'r') as f:
        secured_json = json.load(f)
    secured_json['hostLevelParams']['stack_version'] = '2.2'
    secured_json['configurations']['hdfs-site']['dfs.http.policy'] = 'HTTPS_ONLY'
    secured_json['configurations']['hdfs-site']['dfs.datanode.address'] = '0.0.0.0:10000'
    secured_json['configurations']['hdfs-site']['dfs.datanode.https.address'] = '0.0.0.0:50000'
    self.executeScript((self.COMMON_SERVICES_PACKAGE_DIR + '/scripts/datanode.py'), classname='DataNode', command='stop', config_dict=secured_json, hdp_stack_version=self.STACK_VERSION, target=RMFTestCase.TARGET_COMMON_SERVICES)
    self.assertResourceCalled('File', '/var/run/hadoop/hdfs/hadoop-hdfs-datanode.pid', action=['delete'], not_if='ambari-sudo.sh [RMF_ENV_PLACEHOLDER] -H -E test -f /var/run/hadoop/hdfs/hadoop-hdfs-datanode.pid && ambari-sudo.sh [RMF_ENV_PLACEHOLDER] -H -E pgrep -F /var/run/hadoop/hdfs/hadoop-hdfs-datanode.pid')
    self.assertResourceCalled('Execute', "ambari-sudo.sh su hdfs -l -s /bin/bash -c '[RMF_EXPORT_PLACEHOLDER]ulimit -c unlimited ;  /usr/hdp/current/hadoop-client/sbin/hadoop-daemon.sh --config /usr/hdp/current/hadoop-client/conf stop datanode'", environment={'HADOOP_LIBEXEC_DIR': '/usr/hdp/current/hadoop-client/libexec', }, not_if=None)
    self.assertResourceCalled('File', '/var/run/hadoop/hdfs/hadoop-hdfs-datanode.pid', action=['delete'])
    self.assertNoMoreResources()
