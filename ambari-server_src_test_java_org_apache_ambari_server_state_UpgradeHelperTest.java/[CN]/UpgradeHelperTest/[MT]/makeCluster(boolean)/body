{
  Clusters clusters=injector.getInstance(Clusters.class);
  ServiceFactory serviceFactory=injector.getInstance(ServiceFactory.class);
  String clusterName="c1";
  StackId stackId=new StackId("HDP-2.1.1");
  clusters.addCluster(clusterName,stackId);
  Cluster c=clusters.getCluster(clusterName);
  helper.getOrCreateRepositoryVersion(stackId,c.getDesiredStackVersion().getStackVersion());
  c.createClusterVersion(stackId,c.getDesiredStackVersion().getStackVersion(),"admin",RepositoryVersionState.UPGRADING);
  for (int i=0; i < 4; i++) {
    String hostName="h" + (i + 1);
    clusters.addHost(hostName);
    Host host=clusters.getHost(hostName);
    Map<String,String> hostAttributes=new HashMap<String,String>();
    hostAttributes.put("os_family","redhat");
    hostAttributes.put("os_release_version","6");
    host.setHostAttributes(hostAttributes);
    host.persist();
    clusters.mapHostToCluster(hostName,clusterName);
  }
  c.addService(serviceFactory.createNew(c,"HDFS"));
  c.addService(serviceFactory.createNew(c,"YARN"));
  c.addService(serviceFactory.createNew(c,"ZOOKEEPER"));
  c.addService(serviceFactory.createNew(c,"HIVE"));
  Service s=c.getService("HDFS");
  ServiceComponent sc=s.addServiceComponent("NAMENODE");
  sc.addServiceComponentHost("h1");
  sc.addServiceComponentHost("h2");
  sc=s.addServiceComponent("DATANODE");
  sc.addServiceComponentHost("h2");
  sc.addServiceComponentHost("h3");
  ServiceComponentHost sch=sc.addServiceComponentHost("h4");
  s=c.getService("ZOOKEEPER");
  sc=s.addServiceComponent("ZOOKEEPER_SERVER");
  sc.addServiceComponentHost("h1");
  sc.addServiceComponentHost("h2");
  sc.addServiceComponentHost("h3");
  s=c.getService("YARN");
  sc=s.addServiceComponent("RESOURCEMANAGER");
  sc.addServiceComponentHost("h2");
  sc=s.addServiceComponent("NODEMANAGER");
  sc.addServiceComponentHost("h1");
  sc.addServiceComponentHost("h3");
  s=c.getService("HIVE");
  sc=s.addServiceComponent("HIVE_SERVER");
  sc.addServiceComponentHost("h2");
  Map<String,String> hiveConfigs=new HashMap<String,String>();
  hiveConfigs.put("hive.server2.transport.mode","binary");
  hiveConfigs.put("hive.server2.thrift.port","10001");
  ConfigurationRequest configurationRequest=new ConfigurationRequest();
  configurationRequest.setClusterName(clusterName);
  configurationRequest.setType("hive-site");
  configurationRequest.setVersionTag("version1");
  configurationRequest.setProperties(hiveConfigs);
  final ClusterRequest clusterRequest=new ClusterRequest(c.getClusterId(),clusterName,c.getDesiredStackVersion().getStackVersion(),null);
  clusterRequest.setDesiredConfig(Collections.singletonList(configurationRequest));
  m_managementController.updateClusters(new HashSet<ClusterRequest>(){
{
      add(clusterRequest);
    }
  }
,null);
  HostsType type=new HostsType();
  type.hosts.addAll(Arrays.asList("h1","h2","h3"));
  expect(m_masterHostResolver.getMasterAndHosts("ZOOKEEPER","ZOOKEEPER_SERVER")).andReturn(type).anyTimes();
  type=new HostsType();
  type.hosts.addAll(Arrays.asList("h1","h2"));
  type.master="h1";
  type.secondary="h2";
  expect(m_masterHostResolver.getMasterAndHosts("HDFS","NAMENODE")).andReturn(type).anyTimes();
  type=new HostsType();
  if (clean) {
    type.hosts.addAll(Arrays.asList("h2","h3","h4"));
  }
 else {
    type.unhealthy=Collections.singletonList(sch);
    type.hosts.addAll(Arrays.asList("h2","h3"));
  }
  expect(m_masterHostResolver.getMasterAndHosts("HDFS","DATANODE")).andReturn(type).anyTimes();
  type=new HostsType();
  type.hosts.addAll(Arrays.asList("h2"));
  expect(m_masterHostResolver.getMasterAndHosts("YARN","RESOURCEMANAGER")).andReturn(type).anyTimes();
  type=new HostsType();
  type.hosts.addAll(Arrays.asList("h1","h3"));
  expect(m_masterHostResolver.getMasterAndHosts("YARN","NODEMANAGER")).andReturn(type).anyTimes();
  expect(m_masterHostResolver.getMasterAndHosts("HIVE","HIVE_SERVER")).andReturn(type).anyTimes();
  expect(m_masterHostResolver.getCluster()).andReturn(c).anyTimes();
  replay(m_masterHostResolver);
  return c;
}
