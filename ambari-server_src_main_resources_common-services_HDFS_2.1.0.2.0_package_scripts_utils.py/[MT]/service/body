def service(action=None, name=None, user=None, options='', create_pid_dir=False, create_log_dir=False):
    '\n  :param action: Either "start" or "stop"\n  :param name: Component name, e.g., "namenode", "datanode", "secondarynamenode", "zkfc"\n  :param user: User to run the command as\n  :param options: Additional options to pass to command as a string\n  :param create_pid_dir: Create PID directory\n  :param create_log_dir: Crate log file directory\n  '
    import params
    options = (options if options else '')
    pid_dir = format('{hadoop_pid_dir_prefix}/{user}')
    pid_file = format('{pid_dir}/hadoop-{user}-{name}.pid')
    hadoop_env_exports = {'HADOOP_LIBEXEC_DIR': params.hadoop_libexec_dir, }
    log_dir = format('{hdfs_log_dir_prefix}/{user}')
    if (name == 'nfs3'):
        pid_file = format('{pid_dir}/hadoop_privileged_nfs3.pid')
        custom_export = {'HADOOP_PRIVILEGED_NFS_USER': params.hdfs_user, 'HADOOP_PRIVILEGED_NFS_PID_DIR': pid_dir, 'HADOOP_PRIVILEGED_NFS_LOG_DIR': log_dir, }
        hadoop_env_exports.update(custom_export)
    check_process = ((as_sudo(['test', '-f', pid_file]) + ' && ') + as_sudo(['pgrep', '--pidfile', pid_file]))
    if (action != 'stop'):
        if (name == 'nfs3'):
            Directory(params.hadoop_pid_dir_prefix, mode=493, owner=params.root_user, group=params.root_group)
        else:
            Directory(params.hadoop_pid_dir_prefix, mode=493, owner=params.hdfs_user, group=params.user_group)
        if create_pid_dir:
            Directory(pid_dir, owner=user, recursive=True)
        if create_log_dir:
            if (name == 'nfs3'):
                Directory(log_dir, mode=509, owner=params.root_user, group=params.user_group)
            else:
                Directory(log_dir, owner=user, recursive=True)
    if (params.security_enabled and (name == 'datanode')):
        hadoop_secure_dn_pid_dir = format('{hadoop_pid_dir_prefix}/{hdfs_user}')
        hadoop_secure_dn_pid_file = format('{hadoop_secure_dn_pid_dir}/hadoop_secure_dn.pid')
        if ((not ((params.hdp_stack_version != '') and (compare_versions(params.hdp_stack_version, '2.2') >= 0))) or params.secure_dn_ports_are_in_use):
            user = 'root'
            pid_file = format('{hadoop_pid_dir_prefix}/{hdfs_user}/hadoop-{hdfs_user}-{name}.pid')
        if ((action == 'stop') and ((params.hdp_stack_version != '') and (compare_versions(params.hdp_stack_version, '2.2') >= 0)) and os.path.isfile(hadoop_secure_dn_pid_file)):
            user = 'root'
            try:
                check_process_status(hadoop_secure_dn_pid_file)
                custom_export = {'HADOOP_SECURE_DN_USER': params.hdfs_user, }
                hadoop_env_exports.update(custom_export)
            except ComponentIsNotRunning:
                pass
    hadoop_daemon = format('{hadoop_bin}/hadoop-daemon.sh')
    if (user == 'root'):
        cmd = [hadoop_daemon, '--config', params.hadoop_conf_dir, action, name]
        if options:
            cmd += [options]
        daemon_cmd = as_sudo(cmd)
    else:
        cmd = format('{ulimit_cmd} {hadoop_daemon} --config {hadoop_conf_dir} {action} {name}')
        if options:
            cmd += (' ' + options)
        daemon_cmd = as_user(cmd, user)
    service_is_up = (check_process if (action == 'start') else None)
    File(pid_file, action='delete', not_if=check_process)
    Execute(daemon_cmd, not_if=service_is_up, environment=hadoop_env_exports)
    if (action == 'stop'):
        File(pid_file, action='delete')
