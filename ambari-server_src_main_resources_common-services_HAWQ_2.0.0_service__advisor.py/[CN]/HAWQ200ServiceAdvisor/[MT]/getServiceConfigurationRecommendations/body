def getServiceConfigurationRecommendations(self, stackAdvisor, configurations, clusterData, services, hosts):
    putHdfsSiteProperty = self.putProperty(configurations, 'hdfs-site', services)
    putHdfsSiteProperty('dfs.allow.truncate', 'true')
    if any(((x in services['configurations']) for x in ['hawq-site', 'hdfs-client', 'hawq-sysctl-env'])):
        componentsListList = [service['components'] for service in services['services']]
        componentsList = [item['StackServiceComponents'] for sublist in componentsListList for item in sublist]
        servicesList = [service['StackServices']['service_name'] for service in services['services']]
        hawqMasterHosts = set(self.getHosts(componentsList, 'HAWQMASTER')).union(set(self.getHosts(componentsList, 'HAWQSTANDBY')))
        hawqSegmentHosts = set(self.getHosts(componentsList, 'HAWQSEGMENT'))
        hawqHosts = hawqMasterHosts.union(hawqSegmentHosts)
        numSegments = len(hawqSegmentHosts)
        minHawqHostsMemory = min([host['Hosts']['total_mem'] for host in hosts['items'] if (host['Hosts']['host_name'] in hawqHosts)])
    if ('hawq-site' in services['configurations']):
        hawq_site = services['configurations']['hawq-site']['properties']
        putHawqSiteProperty = self.putProperty(configurations, 'hawq-site', services)
        if (self.isHawqMasterComponentOnAmbariServer(stackAdvisor, services) and ('hawq_master_address_port' in hawq_site)):
            putHawqSiteProperty('hawq_master_address_port', '')
        if (numSegments and ('default_hash_table_bucket_number' in hawq_site) and ('hawq_rm_nvseg_perquery_limit' in hawq_site)):
            factor_min = 1
            factor_max = 6
            limit = int(hawq_site['hawq_rm_nvseg_perquery_limit'])
            factor = (limit / numSegments)
            if (factor < factor_min):
                buckets = limit
            elif (factor > factor_max):
                buckets = (factor_max * numSegments)
            else:
                buckets = (factor * numSegments)
            putHawqSiteProperty('default_hash_table_bucket_number', buckets)
        if (('YARN' in servicesList) and ('yarn-site' in services['configurations'])):
            yarn_site = services['configurations']['yarn-site']['properties']
            for (hs_prop, ys_prop) in self.getHAWQYARNPropertyMapping().items():
                if ((hs_prop in hawq_site) and (ys_prop in yarn_site)):
                    putHawqSiteProperty(hs_prop, yarn_site[ys_prop])
    if ('hawq-sysctl-env' in services['configurations']):
        MEM_THRESHOLD = 33554432
        hawq_sysctl_env = services['configurations']['hawq-sysctl-env']['properties']
        if ('vm.overcommit_memory' in hawq_sysctl_env):
            propertyValue = ('2' if (minHawqHostsMemory >= MEM_THRESHOLD) else '1')
            putHawqSysctlEnvProperty = self.putProperty(configurations, 'hawq-sysctl-env', services)
            putHawqSysctlEnvProperty('vm.overcommit_memory', propertyValue)
    if ('hdfs-client' in services['configurations']):
        MIN_NUM_SEGMENT_THRESHOLD = 3
        hdfs_client = services['configurations']['hdfs-client']['properties']
        if ('output.replace-datanode-on-failure' in hdfs_client):
            propertyValue = ('true' if (numSegments > MIN_NUM_SEGMENT_THRESHOLD) else 'false')
            putHdfsClientProperty = self.putProperty(configurations, 'hdfs-client', services)
            putHdfsClientProperty('output.replace-datanode-on-failure', propertyValue)
