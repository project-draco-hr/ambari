{
  try {
    unitOfWork.begin();
    processCancelledRequestsList();
    if (db.getCommandsInProgressCount() == 0) {
      if (LOG.isDebugEnabled()) {
        LOG.debug("There are no stages currently in progress.");
      }
      return;
    }
    Set<Long> runningRequestIds=new HashSet<Long>();
    List<Stage> stages=db.getStagesInProgress();
    if (LOG.isDebugEnabled()) {
      LOG.debug("Scheduler wakes up");
      LOG.debug("Processing {} in progress stages ",stages.size());
    }
    if (stages.isEmpty()) {
      if (LOG.isDebugEnabled()) {
        LOG.debug("There are no stages currently in progress.");
      }
      return;
    }
    int i_stage=0;
    stages=filterParallelPerHostStages(stages);
    boolean exclusiveRequestIsGoing=false;
    for (    Stage stage : stages) {
      i_stage++;
      long requestId=stage.getRequestId();
      LOG.debug("==> STAGE_i = " + i_stage + "(requestId="+ requestId+ ",StageId="+ stage.getStageId()+ ")");
      Request request=db.getRequest(requestId);
      if (request.isExclusive()) {
        if (runningRequestIds.size() > 0) {
          LOG.debug("Stage requires exclusive execution, but other requests are already executing. Stopping for now");
          break;
        }
        exclusiveRequestIsGoing=true;
      }
      if (runningRequestIds.contains(requestId)) {
        LOG.debug("==> We don't want to process different stages from the same request in parallel");
        continue;
      }
 else {
        runningRequestIds.add(requestId);
        if (!requestsInProgress.contains(requestId)) {
          requestsInProgress.add(requestId);
          db.startRequest(requestId);
        }
      }
      List<ExecutionCommand> commandsToSchedule=new ArrayList<ExecutionCommand>();
      Map<String,RoleStats> roleStats=processInProgressStage(stage,commandsToSchedule);
      boolean failed=false;
      for (      Map.Entry<String,RoleStats> entry : roleStats.entrySet()) {
        String role=entry.getKey();
        RoleStats stats=entry.getValue();
        if (LOG.isDebugEnabled()) {
          LOG.debug("Stats for role:" + role + ", stats="+ stats);
        }
        if (stats.isRoleFailed()) {
          failed=true;
          break;
        }
      }
      if (!failed) {
        failed=hasPreviousStageFailed(stage);
      }
      if (failed) {
        LOG.warn("Operation completely failed, aborting request id:" + stage.getRequestId());
        cancelHostRoleCommands(stage.getOrderedHostRoleCommands(),FAILED_TASK_ABORT_REASONING);
        abortOperationsForStage(stage);
        return;
      }
      List<ExecutionCommand> commandsToStart=new ArrayList<ExecutionCommand>();
      List<ExecutionCommand> commandsToUpdate=new ArrayList<ExecutionCommand>();
      for (      ExecutionCommand cmd : commandsToSchedule) {
        if (cmd.getRole().equals(Role.HIVE_CLIENT.toString()) && cmd.getConfigurations().containsKey(Configuration.HIVE_CONFIG_TAG)) {
          cmd.getConfigurations().get(Configuration.HIVE_CONFIG_TAG).remove(Configuration.HIVE_METASTORE_PASSWORD_PROPERTY);
        }
        processHostRole(stage,cmd,commandsToStart,commandsToUpdate);
      }
      LOG.debug("==> Commands to start: {}",commandsToStart.size());
      LOG.debug("==> Commands to update: {}",commandsToUpdate.size());
      ListMultimap<String,ServiceComponentHostEvent> eventMap=formEventMap(stage,commandsToStart);
      List<ExecutionCommand> commandsToAbort=new ArrayList<ExecutionCommand>();
      if (!eventMap.isEmpty()) {
        LOG.debug("==> processing {} serviceComponentHostEvents...",eventMap.size());
        Cluster cluster=fsmObject.getCluster(stage.getClusterName());
        if (cluster != null) {
          List<ServiceComponentHostEvent> failedEvents=cluster.processServiceComponentHostEvents(eventMap);
          LOG.debug("==> {} events failed.",failedEvents.size());
          for (Iterator<ExecutionCommand> iterator=commandsToUpdate.iterator(); iterator.hasNext(); ) {
            ExecutionCommand cmd=iterator.next();
            for (            ServiceComponentHostEvent event : failedEvents) {
              if (StringUtils.equals(event.getHostName(),cmd.getHostname()) && StringUtils.equals(event.getServiceComponentName(),cmd.getRole())) {
                iterator.remove();
                commandsToAbort.add(cmd);
                break;
              }
            }
          }
        }
 else {
          LOG.warn("There was events to process but cluster {} not found",stage.getClusterName());
        }
      }
      LOG.debug("==> Scheduling {} tasks...",commandsToUpdate.size());
      db.bulkHostRoleScheduled(stage,commandsToUpdate);
      if (commandsToAbort.size() > 0) {
        LOG.debug("==> Aborting {} tasks...",commandsToAbort.size());
        List<Long> taskIds=new ArrayList<Long>();
        for (        ExecutionCommand command : commandsToAbort) {
          taskIds.add(command.getTaskId());
        }
        Collection<HostRoleCommand> hostRoleCommands=db.getTasks(taskIds);
        cancelHostRoleCommands(hostRoleCommands,FAILED_TASK_ABORT_REASONING);
        db.bulkAbortHostRole(stage,commandsToAbort);
      }
      LOG.debug("==> Adding {} tasks to queue...",commandsToUpdate.size());
      for (      ExecutionCommand cmd : commandsToUpdate) {
        if (Role.AMBARI_SERVER_ACTION.name().equals(cmd.getRole())) {
          serverActionExecutor.awake();
        }
 else {
          actionQueue.enqueue(cmd.getHostname(),cmd);
        }
      }
      LOG.debug("==> Finished.");
      if (!configuration.getParallelStageExecution()) {
        return;
      }
      if (exclusiveRequestIsGoing) {
        LOG.debug("Stage requires exclusive execution, skipping all executing any further stages");
        break;
      }
    }
    requestsInProgress.retainAll(runningRequestIds);
  }
  finally {
    LOG.debug("Scheduler finished work.");
    unitOfWork.end();
  }
}
