{
  HttpSession session=req.getSession(true);
  final Logger logger=Logger.getLogger(HiveHistoryMigration.class);
  Connection connectionHuedb=null;
  Connection connectionAmbaridb=null;
  username=req.getParameter("username");
  startDate=req.getParameter("startdate");
  endDate=req.getParameter("enddate");
  instance=req.getParameter("instance");
  logger.info("--------------------------------------");
  logger.info("Hive History query Migration started");
  logger.info("--------------------------------------");
  logger.info("start date: " + startDate);
  logger.info("enddate date: " + endDate);
  logger.info("instance is: " + username);
  logger.info("hue username is : " + instance);
  int maxCountOfAmbariDb, i=0;
  String time=null;
  Long epochTime=null;
  String dirNameforHiveHistroy;
  HiveHistoryQueryImpl hiveHistoryQueryImpl=new HiveHistoryQueryImpl();
  String[] hiveQuery=new String[1000000];
  try {
    connectionHuedb=DataSourceHueDatabase.getInstance(view.getProperties().get("huedrivername"),view.getProperties().get("huejdbcurl"),view.getProperties().get("huedbusername"),view.getProperties().get("huedbpassword")).getConnection();
    hiveQuery=hiveHistoryQueryImpl.fetchFromHue(username,startDate,endDate,connectionHuedb);
    if (hiveQuery[i] == null) {
      logger.info("No queries has been selected acccording to your criteria");
      resp.setContentType("text/html");
      PrintWriter out=resp.getWriter();
      out.println("<br>");
      out.println("<h4>No queries selected according to your criteria</h4>");
    }
 else {
      connectionAmbaridb=DataSourceAmbariDatabase.getInstance(view.getProperties().get("ambaridrivername"),view.getProperties().get("ambarijdbcurl"),view.getProperties().get("ambaridbusername"),view.getProperties().get("ambaridbpassword")).getConnection();
      connectionAmbaridb.setAutoCommit(false);
      for (i=0; hiveQuery[i] != null; i++) {
        float calc=((float)(i + 1)) / hiveQuery.length * 100;
        int progressPercentage=Math.round(calc);
        session.setAttribute(ProgressBarStatus.TASK_PROGRESS_VARIABLE,progressPercentage);
        logger.info("_____________________");
        logger.info("Loop No." + (i + 1));
        logger.info("_____________________");
        logger.info("Hue query that has been fetched" + hiveQuery[i]);
        int id=0;
        id=hiveHistoryQueryImpl.fetchInstanceTablename(view.getProperties().get("ambaridrivername"),connectionAmbaridb,instance);
        logger.info("Table name has been fetched from intance name");
        hiveHistoryQueryImpl.writetoFileQueryhql(hiveQuery[i],ConfFileReader.getHomeDir());
        logger.info(".hql file created in Temp directory");
        hiveHistoryQueryImpl.writetoFileLogs(ConfFileReader.getHomeDir());
        logger.info("Log file created in Temp directory");
        maxCountOfAmbariDb=(hiveHistoryQueryImpl.fetchMaximumIdfromAmbaridb(view.getProperties().get("ambaridrivername"),connectionAmbaridb,id) + 1);
        time=hiveHistoryQueryImpl.getTime();
        epochTime=hiveHistoryQueryImpl.getEpochTime();
        dirNameforHiveHistroy="/user/admin/hive/jobs/hive-job-" + maxCountOfAmbariDb + "-"+ time+ "/";
        logger.info("Directory name where .hql will be saved: " + dirNameforHiveHistroy);
        hiveHistoryQueryImpl.insertRowinAmbaridb(view.getProperties().get("ambaridrivername"),dirNameforHiveHistroy,maxCountOfAmbariDb,epochTime,connectionAmbaridb,id,instance,i);
        if (view.getProperties().get("KerberoseEnabled").equals("y")) {
          logger.info("kerberose enabled");
          hiveHistoryQueryImpl.createDirKerberorisedSecured(dirNameforHiveHistroy,view.getProperties().get("namenode_URI_Ambari"));
          logger.info("Directory created in hdfs");
          hiveHistoryQueryImpl.putFileinHdfsKerborizedSecured(ConfFileReader.getHomeDir() + "query.hql",dirNameforHiveHistroy,view.getProperties().get("namenode_URI_Ambari"));
          hiveHistoryQueryImpl.putFileinHdfsKerborizedSecured(ConfFileReader.getHomeDir() + "logs",dirNameforHiveHistroy,view.getProperties().get("namenode_URI_Ambari"));
        }
 else {
          logger.info("kerberose not enabled");
          hiveHistoryQueryImpl.createDir(dirNameforHiveHistroy,view.getProperties().get("namenode_URI_Ambari"));
          logger.info("Directory created in hdfs");
          hiveHistoryQueryImpl.putFileinHdfs(ConfFileReader.getHomeDir() + "query.hql",dirNameforHiveHistroy,view.getProperties().get("namenode_URI_Ambari"));
          hiveHistoryQueryImpl.putFileinHdfs(ConfFileReader.getHomeDir() + "logs",dirNameforHiveHistroy,view.getProperties().get("namenode_URI_Ambari"));
        }
      }
      connectionAmbaridb.commit();
    }
  }
 catch (  SQLException e) {
    logger.error("Sql exception in ambari database: ",e);
    try {
      connectionAmbaridb.rollback();
      logger.error("Sql statement are Rolledback");
    }
 catch (    SQLException e1) {
      logger.error("Sql rollback exception in ambari database",e1);
    }
  }
catch (  ClassNotFoundException e) {
    logger.error("Class not found :- ",e);
  }
catch (  ParseException e) {
    logger.error("Parse Exception : ",e);
  }
catch (  URISyntaxException e) {
    logger.error("URI Syntax Exception: ",e);
  }
catch (  PropertyVetoException e) {
    logger.error("PropertyVetoException: ",e);
  }
 finally {
    if (connectionAmbaridb != null)     try {
      connectionAmbaridb.close();
    }
 catch (    SQLException e) {
      logger.error("Exception in closing the connection :",e);
    }
  }
  hiveHistoryQueryImpl.deleteFileQueryhql(ConfFileReader.getHomeDir());
  hiveHistoryQueryImpl.deleteFileQueryLogs(ConfFileReader.getHomeDir());
  session.setAttribute(ProgressBarStatus.TASK_PROGRESS_VARIABLE,0);
  logger.info("------------------------------");
  logger.info("Hive History query Migration Ends");
  logger.info("------------------------------");
  resp.setContentType("text/html");
  PrintWriter out=resp.getWriter();
  out.println("<br>");
  out.println("<h4>" + i + " Query has been migrated to  "+ instance+ "</h4>");
}
