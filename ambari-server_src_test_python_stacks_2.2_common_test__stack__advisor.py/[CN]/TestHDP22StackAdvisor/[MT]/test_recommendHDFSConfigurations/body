def test_recommendHDFSConfigurations(self):
    configurations = {'ranger-hdfs-plugin-properties': {'properties': {'ranger-hdfs-plugin-enabled': 'Yes', }, }, 'hdfs-site': {'properties': {'dfs.datanode.data.dir': '/path/1,/path/2,/path/3,/path/4', }, }, }
    clusterData = {'totalAvailableRam': 2048, 'hBaseInstalled': True, 'hbaseRam': 111, 'reservedRam': 128, }
    expected = {'hadoop-env': {'properties': {'namenode_heapsize': '1024', 'namenode_opt_newsize': '128', 'namenode_opt_maxnewsize': '128', }, 'property_attributes': {'dtnode_heapsize': {'maximum': '2048', }, 'namenode_heapsize': {'maximum': '10240', }, }, }, 'hdfs-site': {'properties': {'dfs.datanode.max.transfer.threads': '16384', 'dfs.namenode.safemode.threshold-pct': '1.0f', 'dfs.datanode.failed.volumes.tolerated': '1', 'dfs.namenode.handler.count': '25', 'dfs.datanode.data.dir': '/path/1,/path/2,/path/3,/path/4', }, 'property_attributes': {'dfs.datanode.failed.volumes.tolerated': {'maximum': '4', }, }, }, 'ranger-hdfs-plugin-properties': {'properties': {'ranger-hdfs-plugin-enabled': 'Yes', }, }, }
    services = {'services': [{'StackServices': {'service_name': 'HDFS', 'service_version': '2.6.0.2.2', }, 'components': [{'href': '/api/v1/stacks/HDP/versions/2.2/services/HDFS/components/DATANODE', 'StackServiceComponents': {'advertise_version': 'true', 'cardinality': '1+', 'component_category': 'SLAVE', 'component_name': 'DATANODE', 'custom_commands': [], 'display_name': 'DataNode', 'is_client': 'false', 'is_master': 'false', 'service_name': 'HDFS', 'stack_name': 'HDP', 'stack_version': '2.2', 'hostnames': ['host1'], }, 'dependencies': [], }, {'href': '/api/v1/stacks/HDP/versions/2.2/services/HDFS/components/JOURNALNODE', 'StackServiceComponents': {'advertise_version': 'true', 'cardinality': '0+', 'component_category': 'SLAVE', 'component_name': 'JOURNALNODE', 'custom_commands': [], 'display_name': 'JournalNode', 'is_client': 'false', 'is_master': 'false', 'service_name': 'HDFS', 'stack_name': 'HDP', 'stack_version': '2.2', 'hostnames': ['host1'], }, 'dependencies': [{'href': '/api/v1/stacks/HDP/versions/2.2/services/HDFS/components/JOURNALNODE/dependencies/HDFS_CLIENT', 'Dependencies': {'component_name': 'HDFS_CLIENT', 'dependent_component_name': 'JOURNALNODE', 'dependent_service_name': 'HDFS', 'stack_name': 'HDP', 'stack_version': '2.2', }, }], }, {'href': '/api/v1/stacks/HDP/versions/2.2/services/HDFS/components/NAMENODE', 'StackServiceComponents': {'advertise_version': 'true', 'cardinality': '1-2', 'component_category': 'MASTER', 'component_name': 'NAMENODE', 'custom_commands': ['DECOMMISSION', 'REBALANCEHDFS'], 'display_name': 'NameNode', 'is_client': 'false', 'is_master': 'true', 'service_name': 'HDFS', 'stack_name': 'HDP', 'stack_version': '2.2', 'hostnames': ['host2'], }, 'dependencies': [], }], }], 'configurations': configurations, }
    hosts = {'items': [{'href': '/api/v1/hosts/host1', 'Hosts': {'cpu_count': 1, 'host_name': 'host1', 'os_arch': 'x86_64', 'os_type': 'centos6', 'ph_cpu_count': 1, 'public_host_name': 'host1', 'rack_info': '/default-rack', 'total_mem': 2097152, }, }, {'href': '/api/v1/hosts/host2', 'Hosts': {'cpu_count': 1, 'host_name': 'host2', 'os_arch': 'x86_64', 'os_type': 'centos6', 'ph_cpu_count': 1, 'public_host_name': 'host2', 'rack_info': '/default-rack', 'total_mem': 10485760, }, }], }
    self.stackAdvisor.recommendHDFSConfigurations(configurations, clusterData, services, hosts)
    self.assertEquals(configurations, expected)
    datanode_hostnames = services['services'][0]['components'][0]['StackServiceComponents']['hostnames']
    for i in xrange(10):
        hostname = ('datanode' + `i`)
        datanode_hostnames.append(hostname)
        hosts['items'].append({'href': ('/api/v1/hosts/' + hostname), 'Hosts': {'cpu_count': 1, 'host_name': hostname, 'os_arch': 'x86_64', 'os_type': 'centos6', 'ph_cpu_count': 1, 'public_host_name': hostname, 'rack_info': '/default-rack', 'total_mem': 2097152, }, })
    self.stackAdvisor.recommendHDFSConfigurations(configurations, clusterData, services, hosts)
    self.assertEquals(configurations['hadoop-env']['properties']['namenode_heapsize'], '3072')
    self.assertEquals(configurations['hadoop-env']['properties']['namenode_opt_maxnewsize'], '512')
    self.assertEquals(configurations['hadoop-env']['properties']['namenode_opt_maxnewsize'], '512')
    for i in xrange(11, 30):
        hostname = ('datanode' + `i`)
        datanode_hostnames.append(hostname)
        hosts['items'].append({'href': ('/api/v1/hosts/' + hostname), 'Hosts': {'cpu_count': 1, 'host_name': hostname, 'os_arch': 'x86_64', 'os_type': 'centos6', 'ph_cpu_count': 1, 'public_host_name': hostname, 'rack_info': '/default-rack', 'total_mem': 2097152, }, })
    configurations['hdfs-site']['properties']['dfs.datanode.data.dir'] = '/path1,/path2,/path3,/path4'
    self.stackAdvisor.recommendHDFSConfigurations(configurations, clusterData, services, hosts)
    self.assertEquals(configurations['hadoop-env']['properties']['namenode_heapsize'], '9984')
    self.assertEquals(configurations['hadoop-env']['properties']['namenode_opt_maxnewsize'], '1280')
    self.assertEquals(configurations['hadoop-env']['properties']['namenode_opt_maxnewsize'], '1280')
    for i in xrange(31, 90):
        hostname = ('datanode' + `i`)
        datanode_hostnames.append(hostname)
        hosts['items'].append({'href': ('/api/v1/hosts/' + hostname), 'Hosts': {'cpu_count': 1, 'host_name': hostname, 'os_arch': 'x86_64', 'os_type': 'centos6', 'ph_cpu_count': 1, 'public_host_name': hostname, 'rack_info': '/default-rack', 'total_mem': 2097152, }, })
    self.stackAdvisor.recommendHDFSConfigurations(configurations, clusterData, services, hosts)
    self.assertEquals(configurations['hadoop-env']['properties']['namenode_heapsize'], '5000')
    self.assertEquals(configurations['hadoop-env']['properties']['namenode_opt_maxnewsize'], '1250')
    self.assertEquals(configurations['hadoop-env']['properties']['namenode_opt_maxnewsize'], '1250')
