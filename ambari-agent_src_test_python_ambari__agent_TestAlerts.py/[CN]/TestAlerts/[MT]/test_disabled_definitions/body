def test_disabled_definitions(self):
    test_file_path = os.path.join('ambari_agent', 'dummy_files')
    test_stack_path = os.path.join('ambari_agent', 'dummy_files')
    test_common_services_path = os.path.join('ambari_agent', 'dummy_files')
    test_host_scripts_path = os.path.join('ambari_agent', 'dummy_files')
    ash = AlertSchedulerHandler(test_file_path, test_stack_path, test_common_services_path, test_host_scripts_path)
    ash.start()
    self.assertEquals(1, ash.get_job_count())
    json = {'name': 'namenode_process', 'service': 'HDFS', 'component': 'NAMENODE', 'label': 'NameNode process', 'interval': 6, 'scope': 'host', 'enabled': True, 'uuid': 'c1f73191-4481-4435-8dae-fd380e4c0be1', 'source': {'type': 'PORT', 'uri': '{{hdfs-site/my-key}}', 'default_port': 50070, 'reporting': {'ok': {'text': '(Unit Tests) TCP OK - {0:.4f} response time on port {1}', }, 'critical': {'text': '(Unit Tests) Could not load process info: {0}', }, }, }, }
    pa = PortAlert(json, json['source'])
    ash.schedule_definition(pa)
    self.assertEquals(2, ash.get_job_count())
    json['enabled'] = False
    pa = PortAlert(json, json['source'])
    ash.schedule_definition(pa)
    self.assertEquals(2, ash.get_job_count())
    json['enabled'] = True
    pa = PortAlert(json, json['source'])
    ash.schedule_definition(pa)
    self.assertEquals(3, ash.get_job_count())
