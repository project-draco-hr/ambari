def test_immediate_alert(self):
    test_file_path = os.path.join('ambari_agent', 'dummy_files')
    test_stack_path = os.path.join('ambari_agent', 'dummy_files')
    test_common_services_path = os.path.join('ambari_agent', 'dummy_files')
    test_host_scripts_path = os.path.join('ambari_agent', 'dummy_files')
    ash = AlertSchedulerHandler(test_file_path, test_stack_path, test_common_services_path, test_host_scripts_path, None)
    ash.start()
    self.assertEquals(1, ash.get_job_count())
    self.assertEquals(0, len(ash._collector.alerts()))
    execution_commands = [{'clusterName': 'c1', 'hostName': 'c6401.ambari.apache.org', 'alertDefinition': {'name': 'namenode_process', 'service': 'HDFS', 'component': 'NAMENODE', 'label': 'NameNode process', 'interval': 6, 'scope': 'host', 'enabled': True, 'uuid': 'c1f73191-4481-4435-8dae-fd380e4c0be1', 'source': {'type': 'PORT', 'uri': '{{hdfs-site/my-key}}', 'default_port': 50070, 'reporting': {'ok': {'text': '(Unit Tests) TCP OK - {0:.4f} response time on port {1}', }, 'critical': {'text': '(Unit Tests) Could not load process info: {0}', }, }, }, }, }]
    ash.execute_alert(execution_commands)
    self.assertEquals(1, len(ash._collector.alerts()))
